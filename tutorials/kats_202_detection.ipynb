{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kats 202 - Detection with Kats\n",
    "\n",
    "This tutorial will introduce detection in Kats, including change point detection, outlier detection, and trend detection.  It is a more detailed and in-depth introduction than the one provided in Kats 101.  The table of contents for Kats 202 is as follows:\n",
    "\n",
    "1. Changepoint Detection      \n",
    "    1.1 CUSUMDetector      \n",
    "    1.2 BOCPDetector      \n",
    "    1.3 RobustStatDetector      \n",
    "    1.4 Comparing the Changepoint Detectors      \n",
    "2. Outlier Detection      \n",
    "    2.1 OutlierDetector      \n",
    "    2.2 MultivariateAnomalyDetector      \n",
    "3. Trend Detection      \n",
    "    3.1 MKDetector       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We provide two types of tutorial notebooks\n",
    "- **Kats 101**, basic data structure and functionalities in Kats (this tutorial)  \n",
    "- **Kats 20x**, advanced topics, including advanced forecasting techniques, advanced detection algorithms, `TsFeatures`, meta-learning, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# For Google Colab:\n",
    "!pip install kats\n",
    "!wget https://raw.githubusercontent.com/facebookresearch/Kats/main/kats/data/air_passengers.csv\n",
    "!wget https://raw.githubusercontent.com/facebookresearch/Kats/main/kats/data/multivariate_anomaly_simulated_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:kats.utils.time_series_parameter_tuning requires ax-platform be installed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4b3a83e3e580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kats/kats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconsts\u001b[0m  \u001b[0;31m# noqa # usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m  \u001b[0;31m# noqa # usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetectors\u001b[0m  \u001b[0;31m# noqa # usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraphics\u001b[0m  \u001b[0;31m# noqa # usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m  \u001b[0;31m# noqa # usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kats/kats/detectors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetector_consts\u001b[0m  \u001b[0;31m# noqa # usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhourly_ratio_detection\u001b[0m  \u001b[0;31m# noqa # usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutlier\u001b[0m  \u001b[0;31m# noqa # usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kats/kats/detectors/outlier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeSeriesData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeSeriesIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesian_var\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBayesianVAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVARModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kats/kats/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marima\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbayesian_var\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mharmonic_regression\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mholtwinters\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kats/kats/models/ensemble/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# LICENSE file in the root directory of this source tree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkats_ensemble\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmedian_ensemble\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kats/kats/models/ensemble/ensemble.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeSeriesData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from kats.models import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0marima\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mholtwinters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kats/kats/models/linear_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeSeriesData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/statsmodels/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursive_ls\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile_regression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantReg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_linear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMixedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgenmod\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgenmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m from .genmod.api import (GLM, GEE, OrdinalGEE, NominalGEE, families,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/statsmodels/regression/mixed_linear_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpatsy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_penalties\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPenalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/statsmodels/base/_penalties.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSCAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[1;32m    187\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mSCAD\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0mof\u001b[0m \u001b[0mFan\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mLi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from imp import reload\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from kats.consts import TimeSeriesData\n",
    "reload(logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Changepoint Detection <a class=\"anchor\" id=\"kats_basics\"></a>\n",
    "\n",
    "Changepoint detection tries to identify times when the probability distribution of a stochastic process or time series changes, e.g. the change of mean in a time series. It is one of the most popular detection tasks in time series analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 `CUSUMDetector`**\n",
    "\n",
    "\n",
    "CUSUM is a method to detect an up/down shift of means in a time series. Our implementation has two main steps:\n",
    "\n",
    "\n",
    "1. **Locate the change point:** This is an iterative process where we initialize a change point (in the middle of the time series) and CUSUM time series based on this change point. The next changepoint is the location where the previous CUSUM time series is maximized (or minimized). This iteration continues until either 1) a stable changepoint is found or 2) we exceed the limit number of iterations.\n",
    "2. **Test the change point for statistical significance:** Conduct log likelihood ratio test to test if the mean of the time series changes at the changepoint calculated in Step 1. The null hypothesis is that there is no change in mean.\n",
    "\n",
    "By default, we report a detected changepoint if and only if we reject the null hypothesis in Step 2.  If we want to see all the changepoints, we can use the `return_all_changepoints` parameter in `CUSUMDetector` and set it to `True`.\n",
    "\n",
    "Here are a few additional points worth mentioning:\n",
    "\n",
    "* We assume there is at most one increase change point and at most one decrease change point. You can use the `change_directions` argument in the detector to specify whether you are looking an increase, a decrease, or both (default is both).\n",
    "* We use Gaussian distribution as the underlying model to calculate the CUSUM time series value and conduct the hypothesis test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full set of parameters for the `detector` method in `CUSUMDetector`, all of which are optional and have default values, are as follows: \n",
    "* **threshold**: float, significance level;\n",
    "* **max_iter**: int, maximum iteration in finding the changepoint;\n",
    "* **delta_std_ratio**: float, the mean delta has to be larger than this parameter times std of the data to be considered as a change;\n",
    "* **min_abs_change**: int, minimal absolute delta between mu0 and mu1\n",
    "* **start_point**: int, the start idx of the changepoint, None means the middle of the time series;\n",
    "* **change_directions**: list\\[str], a list contain either or both 'increase' and 'decrease' to specify what type of change to be detected;\n",
    "* **interest_window**: list\\[int, int], a list containing the start and end of the interest window where we will look for a change point. Note that the llr will still be calculated using all data points;\n",
    "* **magnitude_quantile**: float, the quantile for magnitude comparison, if none, will skip the magnitude comparison;\n",
    "* **magnitude_ratio**: float, comparable ratio;\n",
    "* **magnitude_comparable_day**: float, maximal percentage of days can have comparable magnitude to be considered as regression;\n",
    "* **return_all_changepoints**: bool, return all the changepoints found, even the insignificant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by reviewing the basic usage of `CUSUMDetector` from the Kats 101 tutorial.  This simple example is the same one we first introduced there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from kats.detectors.cusum_detection import CUSUMDetector\n",
    "\n",
    "# synthesize data with simulation\n",
    "np.random.seed(10)\n",
    "df_increase_decrease = pd.DataFrame(\n",
    "    {\n",
    "        'time': pd.date_range('2019-01-01', periods=60),\n",
    "        'increase':np.concatenate([np.random.normal(1,0.2,30), np.random.normal(2,0.2,30)]),\n",
    "        'decrease':np.concatenate([np.random.normal(1,0.3,50), np.random.normal(0.5,0.3,10)]),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd = TimeSeriesData(df_increase_decrease.loc[:,['time','increase']])\n",
    "detector = CUSUMDetector(tsd)\n",
    "change_points = detector.detector(change_directions=[\"increase\"])\n",
    "\n",
    "detector.plot(change_points)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd = TimeSeriesData(df_increase_decrease.loc[:,['time','decrease']])\n",
    "detector = CUSUMDetector(tsd)\n",
    "change_points = detector.detector(change_directions=[\"decrease\"])\n",
    "\n",
    "detector.plot(change_points)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not specify which change directions we are looking for using the `change_directions` parameter, `CUSUMDetector` will look for both increases and decreases. In the case below where there is only an increase, it will detect that increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd = TimeSeriesData(df_increase_decrease.loc[:,['time','increase']])\n",
    "detector = CUSUMDetector(tsd)\n",
    "change_points = detector.detector()\n",
    "\n",
    "detector.plot(change_points)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how we can interpret the results of the `CUSUMDetector`.  The return type of the `detector` method is `List[CUSUMChangePoint]`. Most `detector` methods in Kats return this same structure, but some use a subclass of `TimeSeriesChangePoint` with additional information specific to that detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the change point, since we only detected one change point \n",
    "# We can access it with change_points[0] because we only found one change point above\n",
    "change_point = change_points[0]\n",
    "change_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the additional attributes of `CUSUMChangePoint` beyond those of `TimeSeriesChangePoint`:\n",
    "* **direction**: str, the change point direction, either 'increase' or 'decrease';\n",
    "* **cp_index**: int, the index of the change point;\n",
    "* **mu0**: float, mean before the change point;\n",
    "* **mu1**: float, mean after the change point;\n",
    "* **delta**: float, mu1 - mu0;\n",
    "* **llr_int**: float, the log likelihood ratio in interest window, or `inf` if not using an interet window;\n",
    "* **llr**: float, the log likelihood ratio for the full time series;\n",
    "* **regression_detected**: bool, is the change point detected by the algorithm (only relevant when `return_all_changepoints=True`);\n",
    "* **stable_changepoint**: bool, indicates whether the algorithm converged to this point (rather than hitting the iteration limit);\n",
    "* **p_value**: float, p-value for the likelihood ratio test;\n",
    "* **p_value_int**: float, p-value for the likelihood ratio test in interest_window, or `nan` if not using an interest window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explain interest windows in the next section, but for now, notice that since we are not using the `interet_window` parameter in `CUSUMDetector`, the metadata values of `llr_int` and `p_value_int` are `inf` and `nan` respectively.  These values will be populated properly only when we use the `interet_window` parameter in `CUSUMDetector`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Interest Windows\n",
    "\n",
    "Sometimes we want to look for a changepoint in a specific window, which we call an interest window and can be specified using the `interest_window` paramter in `CUSUMDetector`.  Here is what the detector essentially does when using an interest window:\n",
    "\n",
    "1. Do both of the major aforementioned steps of CUSUM (locate the change point and test for signifiance) when only considering the data within the `interest_window`.  By default, continue only if we find a changepoint and reject the null hypothesis.\n",
    "2. Do an additional test for statistical significance on the changepoint found in step 1 using the entire input time series (not just the restriction to `interest_window`).  By default, return this changepoint only if we reject the null hypothesis (unless we set the parameter `return_all_changepoints` to `True`).\n",
    "\n",
    "Since the `CUSUMDetector` can only return one change point in each direction, we can use the `interest_window` parameter narrow the search window in the case that the input time series contains multiple change points.  However, the change point point we return must pass the likelihood ratio test for the entire time series (step 2 above) in addition to passing the likelihood ratio test within the interest window (step 1 above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the first example where we used `CUSUMDetector` to detect an increase.  If we set the `interest_window` to a window that contains the change point that we found above, we will find it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the interest_window between 20th and 40th data point\n",
    "tsd = TimeSeriesData(df_increase_decrease.loc[:,['time','increase']])\n",
    "detector = CUSUMDetector(tsd)\n",
    "change_points = detector.detector(interest_window=[20,40])\n",
    "\n",
    "detector.plot(change_points)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set the `interest_window` to a window that does not find any change point, we will return no change point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the interest_window to last 20 the datapoint and no change point detected\n",
    "tsd = TimeSeriesData(df_increase_decrease.loc[:,['time','increase']])\n",
    "detector = CUSUMDetector(tsd)\n",
    "change_points = detector.detector(interest_window=[40,60])\n",
    "\n",
    "detector.plot(change_points)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Seasonality with Interest Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using an interest window in a time series with seasonality, we can get a false positive.  Here we show how this happens and how we can address the problem.  Let's start by generating a time series with seasonality.  We use a method from our test functions in `CUSUMDetectorTest` to simulate the data or this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.tests.detectors.test_cusum_detection import CUSUMDetectorTest\n",
    "\n",
    "# Here's where we load the seasonal data\n",
    "np.random.seed(1)\n",
    "periodicity=24\n",
    "total_cycles=3\n",
    "seasonal_term = CUSUMDetectorTest.simulate_seasonal_term(\n",
    "    periodicity=periodicity, total_cycles=total_cycles, noise_std=3, harmonics=2\n",
    ")\n",
    "seasonal_term = seasonal_term / seasonal_term.std() * 2\n",
    "residual = np.random.normal(0, 1, periodicity * total_cycles)\n",
    "data = seasonal_term + residual\n",
    "data -= np.min(data)\n",
    "\n",
    "# Load the data into a DataFrame and a TimeSeriesData\n",
    "df_seasonality = pd.DataFrame(\n",
    "    {\n",
    "        \"time\": pd.date_range(\n",
    "            \"2020-01-01\", periods=periodicity * total_cycles, freq=\"H\"\n",
    "        ),\n",
    "        \"seasonality\": data,\n",
    "    }\n",
    ")\n",
    "ts_seasonality = TimeSeriesData(df_seasonality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do detection using the interest window below, we get a false positive.  It is clear that the change point detected below is not a changepoint with respect to an entire series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = CUSUMDetector(ts_seasonality)\n",
    "\n",
    "# Without magnitude comparison\n",
    "change_points = detector.detector(\n",
    "    interest_window=[\n",
    "        periodicity * (total_cycles - 1),\n",
    "        periodicity * total_cycles - 1,\n",
    "    ],\n",
    "    change_directions=[\"increase\"],\n",
    ")\n",
    "\n",
    "detector.plot(change_points)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mitigate false positives here by comparing the magnitude of the time series in the interest window to compare the magnitude of the time series in the interest window to the magnitude of the the time series prior to the interest window.  We need three additional parameters to do this: `magnitude_quantile`, `magnitude_comparable_day`, and `magnitude_ratio`.\n",
    "\n",
    "Here is the procedure for using these parameters to restrict the number of false positives when detecting an increase (when looking for decreases, the time series values are multipled by -1 prior to using this procedure):\n",
    "\n",
    "1) Calculate the `magnitude_quantile` percentile of the timeseries within the `interest_window`.  Call this value `p`    \n",
    "2) Calculate the `magnitude_quantile` percentile over a sliding window of length equal to the length of `interest_window` in the time series prior to `interest_window`.  Count the proportion of intervals for which this percentile is smaller than `p / magnitude_ratio`.  Include this changepoint if and only if this proportion of days exceeds `magnitude_comparable_day`.\n",
    "\n",
    "The values of `magnitude_comparable_day` and `magnitude_ratio` are 0.5 and 1.3 by default, and we only include them below for readability.    When the `magnitude_quantile` is not included as a paramter, we are not doing a magnitude ratio comparison.  Below we have `magnitude_quantile =1` which means the value of `p` in step 1 above is equal to the maximium value of the time series over the `interest_window`.  Then based on the parameters we are using, we will keep a changepoint detected in `interest_window` only if at least half of the intervals prior to `interest_window` with the same length as `interest_window` have a maximum value not exceeding `p/1.3`.\n",
    "\n",
    "As we see below, adding this additional restriction on the change points we detect removes the false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = CUSUMDetector(ts_seasonality)\n",
    "\n",
    "\n",
    "# With magnitude comparison, seasonal false positive not detected\n",
    "change_points = detector.detector(\n",
    "    interest_window=[\n",
    "        periodicity * (total_cycles - 1),\n",
    "        periodicity * total_cycles - 1,\n",
    "    ],\n",
    "    magnitude_quantile=1, # enable magnitude comparison\n",
    "    magnitude_comparable_day=0.5, # this is the default value\n",
    "    magnitude_ratio=1.3, # this is the default value\n",
    "    change_directions=[\"increase\"],\n",
    ")\n",
    "\n",
    "detector.plot(change_points)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Multiple Change Points with Interest Windows\n",
    "In most use cases, there are more than one change point, and you might want to monitor the regressions continuously. Here shows how can we achieve detecting multiple change points over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesize data with simulation\n",
    "np.random.seed(10)\n",
    "df_multi_cps = pd.DataFrame(\n",
    "    {\n",
    "        'time': pd.date_range('2019-01-01', periods=100),\n",
    "        'y':np.concatenate([\n",
    "            np.random.normal(1, 0.2, 30), \n",
    "            np.random.normal(2, 0.2, 30),\n",
    "            np.random.normal(0, 0.2, 20),\n",
    "            np.random.normal(3, 1, 20),\n",
    "        ]),\n",
    "    }\n",
    ")\n",
    "multi_cp_ts = TimeSeriesData(df_multi_cps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the time series using a sliding window, and partition each sliding window into a historical window and a scan window.  The length of each sliding window then will be `historical_window + scan_window`, where `historical_window` and `scan_window` are chosen suitably for the time series being considered.  Run `CUSUMDetector` on the sliding window with the scan window as the interest window.  Repeat and collect all changepoints until the sliding window reaches the end of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_window = 20\n",
    "scan_window = 10\n",
    "step = 5\n",
    "changepoints = []\n",
    "n = len(df_multi_cps)\n",
    "for end_idx in range(historical_window + scan_window, n, step):\n",
    "    tsd = multi_cp_ts[end_idx - (historical_window + scan_window) : end_idx]\n",
    "    changepoints += CUSUMDetector(tsd).detector(interest_window=[historical_window, historical_window + scan_window])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the results.  To use the `plot` method in `CUSUMDetector`, we need to initiate a new `CUSUMDetector` over the entire `multi_cp_ts` and call the `detector` method, but actually using this new detector for anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "detector = CUSUMDetector(multi_cp_ts) # we are not really using this detector\n",
    "detector.detector() # this call to detector is not being used for anything\n",
    "\n",
    "detector.plot(changepoints)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 BOCPDetector**\n",
    "\n",
    "Bayesian Online Change Point Detection (BOCPD) is a method for detecting sudden changes in a time series that persist over time. Our implementation is faithful to [\"Bayesian Online Changepoint Detection\"](https://arxiv.org/abs/0710.3742) (Adams & McKay, 2007).  There are a coupple properties that distinguish BOCPD from the other change point detection methods supported in Kats:\n",
    "\n",
    "* **Online Model:** This detection does not need to know the entire series apriori.  It only need to look a few steps ahead (specified by the `lag` parameter in the `detector` method) to make predictions, and it revises its predictions as new data arrives.  \n",
    "* **Bayesian Model:** The user can specify prior beliefs about the probability of a changepoint (using the `changepoint_prior` paremeter in the `detector` method) and specify the the parameters of the underlying probability model that generates the time series (using the `model_parameters` parameter in the `detector` method).  Right now we support 3 diferent types of underlying probability models (specified using the `model` parameter in the `detector` method).\n",
    "\n",
    "The basic idea of this detection method is uses Bayesian inference to decide if the next point is improbable.  This requires the user to specify (or use default values for) the **probability of a change point** and **underlying predictive model (UPM)** that generates the incoming data points in the time series.  Currently we support three different types of underlying models:\n",
    "1. Normal Distribution (unknown mean, known variance)    \n",
    "2. Trend Change Distribution    \n",
    "3. Poisson Process Model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full set of parameters for the `detector` method in `BOCPDetector`, all of which are optional and have default values, are as follows: \n",
    "\n",
    "* **model**: This specifies the underlying probabilistic model (UPM) that generates the data within each segment.  Currently, allowed models are:\n",
    "    * NORMAL_KNOWN_MODEL: Normal model with variance known. Use this to find level shifts in normally distributed data.\n",
    "    * TREND_CHANGE_MODEL : This model assumes each segment is generated from ordinary linear regression. Use this model to understand changes in slope, or trend in time series.\n",
    "    * POISSON_PROCESS_MODEL: This assumes a Poisson generative model. Use this for count data, where most of the values are close to zero.\n",
    "\n",
    "* **model_parameters**: Model Parameters correspond to specific parameters for a specific model. They are defined in the NormalKnownParameters, TrendChangeParameters, PoissonModelParameters classes.\n",
    "\n",
    "* **lag**: integer referring to the lag in reporting the changepoint. We report the changepoint after seeing \"lag\" number of data points. Higher lag gives greater certainty that this is indeed a changepoint. Lower lag will detect the changepoint faster. This is the trade-off.\n",
    "\n",
    "* **changepoint_prior**: This is a Bayesian algorithm. Hence, this parameter specifies the prior belief on the probability that a given point is a changepoint. For example, if you believe 10% of your data will be a changepoint, you can set this to 0.1.\n",
    "\n",
    "* **threshold**: We report the probability of observing the changepoint at each instant. The actual changepoints are obtained by denoting the points above this threshold to be a changepoint.\n",
    "\n",
    "* **debug**: This surfaces additional information, such as the plots of predicted means and variances, which allows the user to see debug why changepoints were not properly detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** When using the normal distribution for a UPM, there are two ways to pick the prior parameters:      \n",
    "* Use an empirical prior and estimate the parameters from the data (set attribute `empirical=True` in `NormalKnownParameters`)           \n",
    "* Specify the mean and precision of the distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage\n",
    "Here we show the basic usage of `BOCPDetector` to detect mean shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by simulating some data with some change points using the `Simulator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.utils.simulator import Simulator\n",
    "\n",
    "sim = Simulator(n=450, start='2020-01-01', freq='H')\n",
    "ts_bocpd = sim.level_shift_sim(noise=0.05, seasonal_period=1)\n",
    "\n",
    "# plot the simulated data\n",
    "ts_bocpd.plot(cols=['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run BOCPD to find the change points in this simulated time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.detectors.bocpd import BOCPDetector, BOCPDModelType, TrendChangeParameters\n",
    "\n",
    "# Initialize the detector\n",
    "detector = BOCPDetector(ts_bocpd)\n",
    "\n",
    "\n",
    "changepoints = detector.detector(\n",
    "    model=BOCPDModelType.NORMAL_KNOWN_MODEL # this is the default choice\n",
    ")\n",
    "\n",
    "# Plot the data\n",
    "detector.plot(changepoints)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return type of the `detector` method is `List[BOCPDChangePoint]`.  Let's see what that looks like for one of the change points detected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = changepoints[0]\n",
    "cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional attributes of `BOCPDChangePoint` beyond those of `TimeSeriesChangePoint` are:\n",
    "* **detector_type:** This will always be `kats.detectors.bocpd.BOCPDetector`\n",
    "* **model:** The type of the underlying predictive model, either `NORMAL_KNOWN_MODEL`, `TREND_CHANGE_MODEL`, or `POISSON_PROCESS_MODEL`\n",
    "* **ts_name**: the name of the time series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 RobustStatDetector**\n",
    "\n",
    "`RobustStatDetector`, like `CUSUMDetector`, is a change point detection algorithms that finds mean shifts.  It works as follows:\n",
    "1. Smooth the time series using a moving average\n",
    "2. Calculate the differences in the smoothed time series over a fixed number of points (specified by the `comparison_window` parameter in the `detector` method).\n",
    "3. Calculate the z-scores and p-values for the differences calculated in step 2.  Return points where the p-value is smaller than a prescribed threshold (specified by the `p_value_cutoff` parameter in the dtector method`)\n",
    "\n",
    "Unlike `CUSUMDetector`, `RobustStatDetector` can detect multiple change points in a single run.  After a basic example, we will show an example with multiple change points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full set of parameters for the `detector` method in `RobustStatDetector`, all of which are optional and have default values, are as follows: \n",
    "* **p_value_cutoff**: float, the p-value threshold to flag the change point; \n",
    "* **smoothing_window_size**: int, the length of the smoothing window;\n",
    "* **comparison_window**: int, the step of the diff function, i.e. how many data points you want the algorithm look back to make the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by showing how to use `RobustStatDetector` using the same example we initially provided for `CUSUMDetector`.  The usage is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "from kats.detectors.robust_stat_detection import RobustStatDetector\n",
    "\n",
    "tsd = TimeSeriesData(df_increase_decrease.loc[:,['time','increase']])\n",
    "detector = RobustStatDetector(tsd)\n",
    "change_points = detector.detector()\n",
    "\n",
    "detector.plot(change_points)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return type of the `detector` method is `List[RobustStatChangePoint]`.  Let's see what that looks like the change point detected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_point = change_points[0]\n",
    "change_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes of `RobustStatChangePoint` beyond those of `TimeSeriesChangePoint` are:\n",
    "* **metric:** The raw value of the metric at the time of the change point  \n",
    "* **index:** The index of the change point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Changepoints\n",
    "The `RobustStatDetector` can detect multiple change points in one time series by one run. One caution here is that the algorithm will use the full time series to compute the z-score, which means it might impact which change points are detected, e.g. it might not be able to detect a smaller change when there is a bigger change in the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_bocpd = TimeSeriesData(ts_bocpd.to_dataframe())\n",
    "detector = RobustStatDetector(ts_bocpd)\n",
    "changepoints = detector.detector(p_value_cutoff = 5e-3, comparison_window=2)\n",
    "\n",
    "# plot the results\n",
    "detector.plot(changepoints)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd = TimeSeriesData(df_multi_cps)\n",
    "detector = RobustStatDetector(tsd)\n",
    "# even setting the p_value to be 0.1 it cannot detect the first change point, while there are already many FPs in the results\n",
    "changepoints = detector.detector(p_value_cutoff = 1e-1, comparison_window=2)\n",
    "\n",
    "detector.plot(changepoints)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this we can again use the same way in `CUSUMDetector` to detect multiple change points by cutting the long time sereis into several different shorter sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Outlier Detection\n",
    "Outliers in Time Series can cause a lot of issues in downstream processing. Therefore, removing outliers is important in any time series analysis. In Kats, we have three algorithms detecting outliers, one for univariate time series, one for multi-variate time series and one for detecting irregular daily patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 OutlierDetector**\n",
    "We provide the `OutlierDetector` module to detect outliers in time series. Since outliers can cause so many problems in downstream processing, it is important to be able to detect them. `OutlierDetector` also provides functionality to handle or remove outliers once they are found.\n",
    "\n",
    "Our outlier detection algorithm works as follows:\n",
    "* We do a [seasonal decomposition](https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) of the input time series, with additive or multiplicative decomposition as specified (default is additive)\n",
    "* We generate a residual time series by either removing only trend or both trend and seasonality if the seasonality is strong.\n",
    "* We detect points in the residual which are outside 3 times the inter quartile range. This multiplier can be tuned using the `iqr_mult` parameter in `OutlierDetector`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the parameters when initializing `OutlierDetector`:\n",
    "* **data**: `TimeSeriesData` object with the time series\n",
    "* **decomp**: `additive` or 'multiplicative' (default is additive)\n",
    "* **iqr_mult**: `float`, multiplier on inter quartile range is used to classify outliers (default is 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we recap the example from the Kats 101 Tutorial of how to use `OutlierDetector` with the `air_passengers` data set.  Our example manually inserts outliers into the `air_passengers` data set and uses `OutlierDetector` to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the air_passenger data set\n",
    "try: # If running on Jupyter\n",
    "    air_passengers_outlier_df = pd.read_csv(\"../kats/data/air_passengers.csv\")\n",
    "except FileNotFoundError: # If running on colab\n",
    "    air_passengers_outlier_df = pd.read_csv(\"air_passengers.csv\")\n",
    "air_passengers_outlier_df.columns = [\"time\", \"value\"]\n",
    "\n",
    "# manually add outlier on the date of '1950-12-01'\n",
    "air_passengers_outlier_df.loc[air_passengers_outlier_df.time == '1950-12-01','value']*=5\n",
    "# manually add outlier on the date of '1959-12-01'\n",
    "air_passengers_outlier_df.loc[air_passengers_outlier_df.time == '1959-12-01', 'value']*=4\n",
    "\n",
    "# transform the outlier data into `TimeSeriesData` Object\n",
    "air_passengers_outlier_ts = TimeSeriesData(air_passengers_outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import and call `OutlierDetector` on `air_passengers_outlier_ts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.detectors.outlier import OutlierDetector\n",
    "\n",
    "ts_outlierDetection = OutlierDetector(air_passengers_outlier_ts, 'additive') # call OutlierDetector\n",
    "ts_outlierDetection.detector() # apply OutlierDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at the outliers that the algorithum found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_outlierDetection.outliers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After detecting the outlier, we can now easily removal them from the data. Here we will explore two options: \n",
    "- **No Interpolation**: outlier data points will be replaced with **NaN** values\n",
    "- **With Interpolation**: outlier data points will be replaced with **linear interploation** values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_passengers_ts_outliers_removed = ts_outlierDetection.remover(interpolate = False) # No interpolation\n",
    "air_passengers_ts_outliers_interpolated = ts_outlierDetection.remover(interpolate = True) # With interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the difference between these two approaches to removing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8), nrows=1, ncols=2)\n",
    "\n",
    "air_passengers_ts_outliers_removed.to_dataframe().plot(x = 'time',y = 'y_0', ax= ax[0])\n",
    "ax[0].set_title(\"Outliers Removed : No interpolation\")\n",
    "air_passengers_ts_outliers_interpolated.to_dataframe().plot(x = 'time',y = 'y_0', ax= ax[1])\n",
    "ax[1].set_title(\"Outliers Removed : With interpolation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 MultivariateAnomalyDetector**\n",
    "This anomaly detection method is useful to detect anomalies across multiple time series. Anomalies are detected based on deviations from the predicted steady state behavior. The steady state behavior of a system of metrics is predicted by modeling the linear interdependencies between time series using a Vector Autoregression (VAR) model. \n",
    "\n",
    "This approach is especially suited for detecting multivariate anomalies - small anomalies but persistent across a multiple time series.\n",
    "\n",
    "In addition to identifying an anomalous event, this method has useful utilities to flag specific time series that were affected for a high level root cause analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the attributes when initializing `MultivariateAnomalyDetector`:\n",
    "* **data**: TimeSeriesData - Note that data should be deseasonalized and detrended prior to detection\n",
    "* **params**: `VARParams` class initiated with appropriate parameters for the VAR model training\n",
    "* **training_days**: Initial number of days (can be a fraction) to use for training the model. \n",
    "\n",
    "Here are the attributes for the `VARParams` class:\n",
    "* **maxlags**: Maximum number of lags to check for order selection (defaults is 12)\n",
    "* **method**: Estimation method to use (defaults is ordinary least squares (OLS))\n",
    "* **ic**: Information criterion to use for VAR order selection (defaults to None)\n",
    "* **trend**: \n",
    "    * 'c' - add constant (Default), \n",
    "    * 'ct' - constant and trend, \n",
    "    * 'ctt' - constant, linear and quadratic trend,  \n",
    "    * 'n'/'nc' - no constant, no trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we provide an example of how to use `MultivariateAnomalyDetector` that uses the `multivariate_anomaly_simulated_data` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.detectors.outlier import MultivariateAnomalyDetector, MultivariateAnomalyDetectorType\n",
    "from kats.models.var import VARParams\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading and plotting the data as follows.  We can see that at the end of January, there a subsequent movement in metrics 5 and 6 that represents a multivariate anomaly because of the apparent coordination and persistence of these spikes (even though neither spike is particularly large in magnitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the demo data into a TimeSeriesData Object\n",
    "try: # If running on Jupyter\n",
    "    multi_anomaly_df = pd.read_csv(\"../kats/data/multivariate_anomaly_simulated_data.csv\")\n",
    "except FileNotFoundError: # If running on colab\n",
    "    multi_anomaly_df = pd.read_csv(\"multivariate_anomaly_simulated_data.csv\")\n",
    "multi_anomaly_ts = TimeSeriesData(multi_anomaly_df)\n",
    "\n",
    "# Plot the data\n",
    "multi_anomaly_ts.plot(cols=multi_anomaly_ts.value.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train the `MultivariateAnomalyDetector` and plot the results, the first plot gives the time series we previewed above and the second plot gives the overall anomaly scores for the time series.  Since we are using 60 days for training in this example, the first 60 days are not included in the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "params = VARParams(maxlags=2)\n",
    "d = MultivariateAnomalyDetector(multi_anomaly_ts, params, training_days=60)\n",
    "anomaly_score_df = d.detector()\n",
    "d.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the overall anomaly score spikes around the end of January.  We can list the specific dates that are anomalies using the `get_anomaly_timepoints` function and specifying the desired significance level (here we use 5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = d.get_anomaly_timepoints(alpha=0.05)\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using the `get_anomalous_metrics` function we can look at the at the top metrics that are contributing to a multivariate anomaly.  In the case of the anomaly times we found here, we can verify that the largest anomaly scores come from metrics 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get_anomalous_metrics(anomalies[0], top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Trend detection\n",
    "Trend detection tries to identify significant and prolonged changes in a time series.  Rather than identifying change points, trend detection identifies windows of gradual and prolonged change.\n",
    "\n",
    "`MKDetector` is the trend detection algorithm that we include in Kats, which is based on the non-parametric Mann-Kendall (MK) Test.  What the `MKDetector` essentially does is apply a MK to a window of fied size (specified by the `window_size` argument in the `detector` method) and return the end point of each window for which this test is statistically significant.  Trend windows are detected based on the monotonicity of the increases or decreases in the time series in the window, not the magnitude of the change in the value of the time series over the window.  \n",
    "\n",
    "The test statistic for the MK test is called the Kendall's Tau Coefficient, which ranges from -1 to 1.  \n",
    "* A Tau coefficient of -1 indicates a perfectly monotonic decline\n",
    "* A Tau coefficient of 1 indicates a perfectly monotonic increase\n",
    "* A coefficient of 0 indicates there is no directional trend\n",
    "\n",
    "The MK Test converts the Tau Coefficient (along with the value of `window_size`) to a p-value, and we return change points for which the p-value is smaller than `alpha` (0.05 by default).  For additional control of the intensity of the trends we return, we can use the `threshold` attribute in `MKDetector` to put an absolute lower-bound threshold on the value of the Tau Coeffficients for trend windows that are returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These are the parameters to use when initializing `MKDetector`:\n",
    "* **data**: TimeSeriesData, this is time series data at one-day granularity. This time series can be either univariate or multivariate. We require more than training_days points in each time series.\n",
    "* **threshold**: float, threshold for trend intensity; higher threshold gives trend with high intensity (0.8 by default)\n",
    "* **alpha**: float, significance level (0.05 by default)\n",
    "\n",
    "The full set of parameters for the `detector` method in `MKDetector`, all of which are optional and have default values, are as follows: \n",
    "* **window_size**: int, the number of look back days for checking trend persistence (20 days by default)\n",
    "* **training_days**: int, the number of days for time series smoothing; should be greater or equal to window_size (None by default). If training_days is None, we will perform trend detection on the whole time series; otherwise, we will perform trend detection only for the anchor point using the previous training_days data.\n",
    "\n",
    "* **direction**: string, the direction of the trend to be detected, choose from {\"down\", \"up\", \"both\"}  (\"both\" by default)\n",
    "\n",
    "* **freq**: str, the type of seasonality shown in the time series, choose from {'weekly','monthly','yearly'} (None by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by simulating some data with weekly seasonality using the `Simulator` class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.utils.simulator import Simulator\n",
    "\n",
    "sim = Simulator(n=365, start='2020-01-01', freq='D')\n",
    "tsd = sim.trend_shift_sim(noise=200, seasonal_period=7, seasonal_magnitude=0.007,\n",
    "                              cp_arr=[250], intercept = 10000, trend_arr=[40,-20])\n",
    "\n",
    "# plot the simulated data\n",
    "tsd.plot(cols=['value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `MKDetector` detector to look for a downward trend.  Because the input data has weekly seasonality, will want to smooth the time series using a 7-day rolling average prior to running the detection algorithm.  We can do this by `freq = 'weekly'` in the `detector` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.detectors.trend_mk import MKDetector\n",
    "\n",
    "detector = MKDetector(data=tsd, threshold=.8)\n",
    "# run detector\n",
    "detected_time_points = detector.detector(direction='down', window_size=20, freq='weekly')\n",
    "# plot the results\n",
    "detector.plot(detected_time_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the results\n",
    "The return type of the `detector` method is `List[MKChangePoint]`.  Each `MKChangePoint` returned is the end of an increasing or decreasing trend window of duration `window_size`.  In our example, we are looking for decreasing trend windows of length 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = detected_time_points[0]\n",
    "cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes of `MKChangePoint` beyond those of `TimeSeriesChangePoint` are:\n",
    "* **detector_type:** Always `kats.detectors.trend_mk.MKDetector`     \n",
    "* **is_multivariate:** Whether this is a change point for a multivariate time seires\n",
    "* **trend_direction:** Direction of the trend, either 'increasing' or 'decreasing'\n",
    "* **Tau:** kendall's Tau value for the change point.  This is a float for univariate time series and a dictionary for multivariate time series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "metadata": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
